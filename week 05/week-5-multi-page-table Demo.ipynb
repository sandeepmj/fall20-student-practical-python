{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multipage Tables Scrape Demo\n",
    "\n",
    "You're often going to encounter data and tables that is spread across hundreds if not thousands of pages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We're going to scrape as a demo a table that runs across several pages on this mock website.\n",
    "\n",
    "```https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To capture your target information into a single CSV file will require the use of many of the foundational skills we've covered, including:\n",
    "\n",
    "- ```delays```\n",
    "- ```conditional logic```\n",
    "- ```while loops```\n",
    "- ```BeautifulSoup```\n",
    "\n",
    "\n",
    "And we'll explore a few new functional Python methods today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scraping Strategies\n",
    "\n",
    "- How do we approach this scrape?\n",
    "- What pattern do we see?\n",
    "- How do we capture a table on a single page?\n",
    "- How do we capture a sequence of tables?\n",
    "- How we navigate from page 1 to the subsequent pages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "from bs4 import BeautifulSoup  ## web scraping\n",
    "import requests ## request html for a page(s)\n",
    "import csv ## read or write to csv\n",
    "import pandas as pd ## pandas to work with data\n",
    "import re ## regular express in one of our functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Single Table Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "##scrape url website\n",
    "url = \"https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page1.html\"\n",
    "page = requests.get(url)\n",
    "print(page.status_code)  ## should print 200. checks http response code status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "## turn into soup\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "print(type(soup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "## MUST turn html into a string\n",
    "html = soup.prettify()\n",
    "print(type(html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                 Animal  Weight(kg)    Type\n",
       " 0            Blue whale      136000  Marine\n",
       " 1         Bowhead whale      100000  Marine\n",
       " 2             Fin whale       70000  Marine\n",
       " 3  Southern right whale       45000  Marine\n",
       " 4        Humpback whale       30000  Marine]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## use Pandas to read tables on page\n",
    "df = pd.read_html(html)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Do we want the first table?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Animal</th>\n",
       "      <th>Weight(kg)</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blue whale</td>\n",
       "      <td>136000</td>\n",
       "      <td>Marine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bowhead whale</td>\n",
       "      <td>100000</td>\n",
       "      <td>Marine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fin whale</td>\n",
       "      <td>70000</td>\n",
       "      <td>Marine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Southern right whale</td>\n",
       "      <td>45000</td>\n",
       "      <td>Marine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Humpback whale</td>\n",
       "      <td>30000</td>\n",
       "      <td>Marine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Animal  Weight(kg)    Type\n",
       "0            Blue whale      136000  Marine\n",
       "1         Bowhead whale      100000  Marine\n",
       "2             Fin whale       70000  Marine\n",
       "3  Southern right whale       45000  Marine\n",
       "4        Humpback whale       30000  Marine"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## store it into a copy called animals_df\n",
    "animals_df = df[0].copy() \n",
    "animals_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But we want to scrape multiple pages\n",
    "2 ways to build a list of urls that we have to navigate to:\n",
    "\n",
    "1. Placeholders\n",
    "2. f-strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1. Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## How is it different?\n",
    "url = \"https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page{}.html\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Placeholders\n",
    "\n",
    "<img src=\"../support_files/placeholder1.png\" style=\"width:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Placeholders\n",
    "\n",
    "<img src=\"../support_files/placeholder2.png\" style=\"width:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Placeholders\n",
    "\n",
    "<img src=\"../support_files/placeholder3.png\" style=\"width:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Filling the Placeholder\n",
    "\n",
    "### We use ```.format()``` to fill in values into the ```{}```placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://www.example{}.html'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## here's our base url\n",
    "## here's our base url\n",
    "base_link = \"http://www.example{}.html\"\n",
    "base_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "http://www.example1.html\n",
      "2\n",
      "http://www.example2.html\n",
      "3\n",
      "http://www.example3.html\n",
      "4\n",
      "http://www.example4.html\n",
      "5\n",
      "http://www.example5.html\n",
      "6\n",
      "http://www.example6.html\n"
     ]
    }
   ],
   "source": [
    "## Using a ```for loop```\n",
    "\n",
    "all_urls_fl = []\n",
    "for url_number in range(1,7):\n",
    "    print(url_number)\n",
    "    print(base_link.format(url_number))\n",
    "    all_urls_fl.append(base_link.format(url_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.example1.html',\n",
       " 'http://www.example2.html',\n",
       " 'http://www.example3.html',\n",
       " 'http://www.example4.html',\n",
       " 'http://www.example5.html',\n",
       " 'http://www.example6.html']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_urls_fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.example1.html',\n",
       " 'http://www.example2.html',\n",
       " 'http://www.example3.html',\n",
       " 'http://www.example4.html',\n",
       " 'http://www.example5.html',\n",
       " 'http://www.example6.html']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using list comprehension\n",
    "all_urls_lc = [base_link.format(url_number) for url_number in range(1,7)]\n",
    "all_urls_lc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. Using f-strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "burl = \"http://www.example\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Using a ```for loop```\n",
    "all_urls_fs = []\n",
    "for url_number in range(1,7):\n",
    "    temp_url = f\"{burl}{url_number}.html\"\n",
    "    all_urls_fs.append(temp_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://www.example1.html',\n",
       " 'http://www.example2.html',\n",
       " 'http://www.example3.html',\n",
       " 'http://www.example4.html',\n",
       " 'http://www.example5.html',\n",
       " 'http://www.example6.html']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_urls_fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back to our scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page{}.html'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## let's remind ourselves of url variable's value\n",
    "\n",
    "url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We know we need a placeholder value of upto ```4```\n",
    "## Let's create a variable called  ```total_pages``` to match number of pages on site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## total pages to scrape\n",
    "total_pages = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "200\n",
      "200\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "## Let's write the for loop\n",
    "## but instead of storing into a list, we just feed it directly to our placeholder\n",
    "## we want to just scape each page\n",
    "for url_number in range(1,total_pages):\n",
    "    link = url.format(url_number)\n",
    "    site = requests.get(link)\n",
    "    print(site.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# We have a problem...\n",
    "\n",
    "### We're hitting the server way too fast. We have to add a delay before we proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Let's import the required libaries to create a delay\n",
    "from random import randrange ##  allows us to randomize numbers library\n",
    "import time ## time tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "snoozing for 3 seconds before next url\n",
      "200\n",
      "snoozing for 3 seconds before next url\n",
      "200\n",
      "snoozing for 5 seconds before next url\n",
      "200\n",
      "snoozing for 5 seconds before next url\n"
     ]
    }
   ],
   "source": [
    "## Let's run our code again but with appropriate delay\n",
    "\n",
    "for url_number in range(1,total_pages):\n",
    "    link = url.format(url_number)\n",
    "    site = requests.get(link)\n",
    "    print(site.status_code)\n",
    "    snooze = randrange(3,6)\n",
    "    print(f\"snoozing for {snooze} seconds before next url\")\n",
    "    time.sleep(snooze)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working Around Errors\n",
    "\n",
    "When you scrape hundreds of pages, there's chance that one of the URLs might be a dud.\n",
    "\n",
    "We can set up a error control to see what kind of responses we get:\n",
    "\n",
    "```<Response [200]>``` means website is accessible.\n",
    "\n",
    "```<Response [404]>``` means broken link or no page on content.\n",
    "\n",
    "In that case, your whole code might break and you'll have to figure out where it broke.\n",
    "\n",
    "We can make that easier with conditional logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n",
      "oh no, https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page0.html returned:  404\n",
      "200\n",
      "snoozing for 4 seconds before next url\n",
      "200\n",
      "snoozing for 3 seconds before next url\n",
      "200\n",
      "snoozing for 5 seconds before next url\n",
      "200\n",
      "snoozing for 3 seconds before next url\n"
     ]
    }
   ],
   "source": [
    "## CHECK FOR ERROR\n",
    "for url_number in range(0,total_pages):\n",
    "    link = url.format(url_number)\n",
    "    site = requests.get(link)\n",
    "    print(site.status_code)\n",
    "    if site.status_code == 200:\n",
    "        soup = BeautifulSoup(site.content, \"html.parser\")\n",
    "        snooze = randrange(3,6)\n",
    "        print(f\"snoozing for {snooze} seconds before next url\")\n",
    "        time.sleep(snooze)\n",
    "        \n",
    "    else: \n",
    "        print(f\"oh no, {link} returned: \", site.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# All in One Step\n",
    "\n",
    "Because we are using a  ```for loop``` that cycles through each link to do multiple steps on our target data, we need to have it done as one step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n",
      "oh no, https://sandeepmj.github.io/scrape-example-page/heaviest-animals-page0.html returned:  404\n",
      "200\n",
      "snoozing for 5 seconds before next url\n",
      "200\n",
      "snoozing for 5 seconds before next url\n",
      "200\n",
      "snoozing for 4 seconds before next url\n",
      "200\n",
      "snoozing for 5 seconds before next url\n"
     ]
    }
   ],
   "source": [
    "## Combined url timed nav with table scrape\n",
    "df_all = []\n",
    "for url_number in range(0,total_pages):\n",
    "    link = url.format(url_number)\n",
    "    site = requests.get(link)\n",
    "    print(site.status_code)\n",
    "    if site.status_code == 200:\n",
    "        soup = BeautifulSoup(site.content, \"html.parser\")\n",
    "        html = soup.prettify()  ## turn soup into html \n",
    "        df = pd.read_html(html) ## turn html table into a df using pandas\n",
    "        df_all.append(df[0]) ## append that first table to a list\n",
    "        snooze = randrange(3,6)\n",
    "        print(f\"snoozing for {snooze} seconds before next url\")\n",
    "        time.sleep(snooze)\n",
    "        \n",
    "    else: \n",
    "        print(f\"oh no, {link} returned: \", site.status_code)\n",
    "print(\"Done scraping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                 Animal  Weight(kg)    Type\n",
       " 0            Blue whale      136000  Marine\n",
       " 1         Bowhead whale      100000  Marine\n",
       " 2             Fin whale       70000  Marine\n",
       " 3  Southern right whale       45000  Marine\n",
       " 4        Humpback whale       30000  Marine,\n",
       "                  Animal  Weight(kg)    Type\n",
       " 0            Gray whale       28500  Marine\n",
       " 1  Northern right whale       23000  Marine\n",
       " 2             Sei whale       20000  Marine\n",
       " 3         Bryde's whale       16000  Marine\n",
       " 4  Baird's beaked whale       11380  Marine,\n",
       "                       Animal  Weight(kg)         Type\n",
       " 0                Minke whale        7500       Marine\n",
       " 1  Northern bottlenose whale        6500       Marine\n",
       " 2     Gervais's beaked whale        5600       Marine\n",
       " 3           African elephant        4800  Terrestrial\n",
       " 4               Killer whale        3988       Marine,\n",
       "                      Animal  Weight(kg)         Type\n",
       " 0              Hippopotamus        3750  Terrestrial\n",
       " 1            Asian elephant        3178  Terrestrial\n",
       " 2     Cuvier's beaked whale        2701       Marine\n",
       " 3  Short-finned pilot whale        2200       Marine\n",
       " 4          White rhinoceros        2175  Terrestrial]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## FUNCTION to download individual dataframes in a list as a single csv\n",
    "def combine_tables(list_name,filename):\n",
    "  '''\n",
    "  Takes dataframes in a list and combines into a single CSV.\n",
    "  Tables must have identical column headers and order\n",
    "  Arguments: name of list produced by tabula and the CSV name you want (in quotes as a string)\n",
    "  '''\n",
    "  dataframes = [pd.DataFrame(a_table) for a_table in list_name] ## list comprehension to turn each tabula table into a dataframe\n",
    "  df = pd.concat(dataframes) ## join/concat all the dataframes into one dataframe\n",
    "  df.reset_index(inplace = True, drop = True) ## reset index, drop what was there before\n",
    "  df.to_csv(filename, encoding='utf-8', index=False) ## convert that single dataframe into a csv\n",
    "#   files.download(filename) ## download it\n",
    "  print(f\"{filename} is in your current folder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heavy_animals.csv is in your current folder!\n"
     ]
    }
   ],
   "source": [
    "## CALL THE FUNCTION\n",
    "combine_tables(df_all, \"heavy_animals.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
