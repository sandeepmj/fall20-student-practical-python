{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QwY_aPURcTp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The problem?\n",
    "\n",
    "- Endless amounts of unstructured data found in emails, tweets, letters, memos, etc.\n",
    "- Even in transcripts\n",
    "- How can we make sense of all this data?\n",
    "- How can we 'easily' find relevant information for our reporting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The solution?\n",
    "- Computer programming to process all that text using **natural language processing**!\n",
    "- <a href=\"https://machinelearningmastery.com/natural-language-processing/\">Learn more</a> about the complexity and the history of NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Journalism examples\n",
    "\n",
    "- <a href=\"http://doctors.ajc.com/part_1_license_to_betray/\">License to betray</a> – Finding word stems and roots to uncover abuse.\n",
    "- <a href=\"https://www.revealnews.org/article/federal-judges-rulings-favored-companies-in-which-he-owned-stock/\">Federal judge’s rulings favored companies in which he owned stock</a> – Finding all stock owned by judges in disclosure forms and comparing to caseloads.\n",
    "- <a href=\"https://www.latimes.com/local/cityhall/la-me-crime-stats-20151015-story.html\">LAPD underreported serious assaults, skewing crime stats for 8 years</a> – Text classification analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The tools\n",
    "\n",
    "- Spacy v. NLTK\n",
    "- NLTK launched in 2001, Spacy in 2015\n",
    "- NLTK is now bloated and complex, requiring many steps to deal with many changes etc.\n",
    "- Spacy is lean and modern, and can compute some text 4x to 20x faster than NLTK.\n",
    "- Spacy does **nearly** everything that NLTK does, but better.\n",
    "- NLTK, however, is still the library of choice for sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Working with Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ly8WKOVPYV9-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 1. Install Spacy\n",
    "\n",
    "If this first time ever using spacy on this computer, you must first do either the ```!conda install``` or ```!pip install```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBpamsrWRcTr",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Conda install or...\n",
    "# !conda install -c conda-forge spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9947,
     "status": "ok",
     "timestamp": 1605068927258,
     "user": {
      "displayName": "Sandeep Junnarkar",
      "photoUrl": "",
      "userId": "15658885848901522782"
     },
     "user_tz": 300
    },
    "id": "a-S6NorkRcTu",
    "outputId": "4307f275-5b08-45d2-b94c-273cdd5554dd",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## pip install\n",
    "!pip install -U spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdUvDJNDRcTy",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## import libary.\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8xpwtwPRcT1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Troubleshoot here if problems with setup:\n",
    "https://github.com/explosion/spacy-models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KcPisnnlRcT1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 2. Install language model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7336,
     "status": "ok",
     "timestamp": 1605069048515,
     "user": {
      "displayName": "Sandeep Junnarkar",
      "photoUrl": "",
      "userId": "15658885848901522782"
     },
     "user_tz": 300
    },
    "id": "9L770hGkZePT",
    "outputId": "4815be5b-47c2-493a-b4e8-db38883fc16b",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVP20HngRcT8"
   },
   "source": [
    "### Place English libary into a ```nlp``` pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqDMPPHxRcT8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## build nlp pipeline (a function will tokenize, parse and ner for us)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 298,
     "status": "ok",
     "timestamp": 1605069097627,
     "user": {
      "displayName": "Sandeep Junnarkar",
      "photoUrl": "",
      "userId": "15658885848901522782"
     },
     "user_tz": 300
    },
    "id": "R0G92vbFFFlU",
    "outputId": "f6a96803-3e31-433f-bfa0-fa9c6a63e045",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## what type of object is nlp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Step 3. Text analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yy8E66pRRcT5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### Sampel English text:\n",
    "text = u'''\\\n",
    "On May 10, 2011, Microsoft announced its acquisition of Skype Technologies,\\\n",
    "creator of the VoIP service Skype, for $8.5 billion.\\\n",
    "Microsoft is headquartered near Seattle Washington while Skype remains in Palo Alto, California.\\\n",
    "Sandeep Junnarkar got this from Wikipedia.\\\n",
    "But he'd rather head to Paris, France to see the Mona Lisa at the Louvre.\\\n",
    "Mount Washington, which is really Agiocochook, is the highest peak in the Northeastern United States\\\n",
    "at 6,288.2 ft and the most topographically prominent mountain east\\\n",
    "of the Mississippi River. It's not in Mississippi.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZQdKpPKRcT_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenize our text\n",
    "\n",
    "- Tokenizing is always the first step in text analysis. \n",
    "- It breaks all text into isolated but related units (including spaces, symbols, punctuation, numbers, words etc.)\n",
    "- However, it retains the connection between all the words, sentences, and paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHeDiZnRRcUA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## let's run the nlp function and create a spacy doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1605069248572,
     "user": {
      "displayName": "Sandeep Junnarkar",
      "photoUrl": "",
      "userId": "15658885848901522782"
     },
     "user_tz": 300
    },
    "id": "y1aV38l6RcUC",
    "outputId": "bbbf5311-29a2-4ed6-df76-42d4df1142e6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## what type of data is it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zJ_bNTzRcUF",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## show each token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stop Words\n",
    "\n",
    "- These are common words that add no additional meaning to our analysis.\n",
    "- Words like ```the```, ```and``` and ```any```.\n",
    "- Spacy has just over 320 ```stop words``` in its defalt library.\n",
    "- Read more on <a href=\"https://medium.com/@saitejaponugoti/stop-words-in-nlp-5b248dadad47\">stop words</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## show all default stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## check if a word (have, near, be) is a stop word \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## how many do stop words do we have?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Add your own stop word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## CHECK IF 'lol' is a stop word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## how many do stop words do we have now?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Remove a stop word from list because it is relevant.\n",
    "## notice the word \"empty\" is a stop word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## CHECK IF 'empty' is a stop word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of speech\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## print all parts of speech words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Step 4. Named Entity Recognition (NER)\n",
    "\n",
    "#### Spacy easily returns the words that matter to us like names of companies, people, places, art works, numbers, etc.\n",
    "\n",
    "- ```.ents``` ------------> Finds all entities in doc spacy object.\n",
    "\n",
    "- ```ent.text``` ------------> The actual text.\n",
    "\n",
    "- ```ent.label``` ------------> A numeric code for the entity.\n",
    "\n",
    "- ```ent.label_``` ------------> The word's entity category.\n",
    "\n",
    "- ```spacy.explain(ent.label_)``` ---------> A description of the category.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find all entities\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find all entities with their label\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find all entities with their label and label descriptors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a CSV that holds all the organizations/companies in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find all entities and place in a list using list comprehension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Turn the two lists into a dictionary using a for loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Turn the two lists into a dictionary using a list comprehension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the previous lists hold all entities. \n",
    "## let's narrow them down to the orgs/companies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What data types are these?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's make sure all the key and value pairs are strings \n",
    "## instead of spacy objects so we can move them into a df and csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm key, value are both strings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deduplicate a dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## use pandas to write to csv file\n",
    "filename = \"test_entities.csv\"\n",
    "df = pd.DataFrame(orgs_only) ## we turn our life dict into a dataframe which we're call df\n",
    "df.to_csv(filename, encoding='utf-8', index=False)\n",
    "\n",
    "print(f\"{filename} is in your project folder!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_aeW2aMRcUK",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## function to find entities\n",
    "def show_entities(my_text):\n",
    "    each_token = \"Token\"\n",
    "    entity_type = \"Entity\"\n",
    "    entity_def = \"Entity Defined\"\n",
    "    print(f\"{each_token:{30}}{entity_type:{15}}{entity_def}\")\n",
    "    if my_text.ents:\n",
    "        for word in doc.ents:\n",
    "            print(f\"{word.text:{30}} {word.label_:{15}} {str(spacy.explain(word.label_))}\")\n",
    "    else:\n",
    "        print(\"There are no entities in this text\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token.text.replace(u'\\xa0', ' ') for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1605069904247,
     "user": {
      "displayName": "Sandeep Junnarkar",
      "photoUrl": "",
      "userId": "15658885848901522782"
     },
     "user_tz": 300
    },
    "id": "oTNWHsd_RxW9",
    "outputId": "b74688d0-6dd8-4078-d242-161c962139b7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## show entities in my english sentence\n",
    "show_entities(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter  ## a package that helps us count up frequency\n",
    "## Counter(some_variable)\n",
    "## variable_name.most.common(some_number)\n",
    "\n",
    "#remove stopwords and punctuations\n",
    "words = [token.text for token in doc if token.is_stop != True and token.is_punct != True and token.text != '\\xa0']\n",
    "word_freq = Counter(words)\n",
    "common_words = word_freq.most_common(25)  ## use most.common()\n",
    "print (common_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nkzIHWcRcUM",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Install other languages\n",
    "#### Other languages can be found at https://spacy.io/usage/models\n",
    "\n",
    "#### Disclaimer: Language models are built by open source communities. English and German are the most advanced language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QC8HluMERcUN",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Spanish language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuHEEuUDSXPa",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## !python install the library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qmjeb7LYRcUQ",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## import the library and create nlp pipleline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEfgJHvERcUW",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### Sample Spanish Text (sorry!)\n",
    "stext = \"\"\"\n",
    "El 10 de mayo de 2011, Microsoft anunció la adquisición de Skype Technologies, \\\n",
    "creador del servicio de VoIP Skype, por $ 8.5 mil millones. \\\n",
    "Microsoft tiene su sede cerca de Seattle Washington, mientras que Skype permanece en Palo Alto, California. \\\n",
    "Sandeep Junnarkar obtuvo esto de Wikipedia. \\\n",
    "Pero preferiría ir a París, Francia, para ver la Mona Lisa en el Louvre. \\\n",
    "Mount Washington, que en realidad es Agiocochook, es el pico más alto del noreste de Estados Unidos.\n",
    "a 6.288,2 pies y la montaña más prominente topográficamente al este \\\n",
    "del río Mississippi.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eg767ypnRcUY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## tokenize and show parts of speech for each token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dt67pTWYGmVf",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## show the tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onKjJi8ORcUa",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## show entities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MSEF5iHleMY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Chinese language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20143,
     "status": "ok",
     "timestamp": 1605070302857,
     "user": {
      "displayName": "Sandeep Junnarkar",
      "photoUrl": "",
      "userId": "15658885848901522782"
     },
     "user_tz": 300
    },
    "id": "uLTao2Lmlgng",
    "outputId": "855c8c8b-7e3d-4075-a367-cb49cb7db4e2",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## !python install the library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tNRPxSFDlr7J",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## import the library and create nlp pipleline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZw79PW1qzTc",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### Sample Chinese Text (sorry!)\n",
    "ctext = '''\n",
    "2011年5月10日，微軟宣布收購Skype Technologies，\n",
    "VoIP服務的創造者，價格為85億美元。\n",
    "微軟總部位於華盛頓州西雅圖市附近，而Skype仍位於加利福尼亞州帕洛阿爾托。\\\n",
    "Sandeep Junnarkar從Wikipedia獲得了此信息。\\\n",
    "但他寧願前往法國巴黎在羅浮宮看《蒙娜麗莎》。\\\n",
    "華盛頓山（實際上是Agiocochook）是美國東北部的最高峰\\\n",
    "位於6,288.2英尺，是東面地形最突出的山脈\\\n",
    "密西西比河。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFzWDLcBpf8-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## create a spacy doc object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 339,
     "status": "ok",
     "timestamp": 1605070781700,
     "user": {
      "displayName": "Sandeep Junnarkar",
      "photoUrl": "",
      "userId": "15658885848901522782"
     },
     "user_tz": 300
    },
    "id": "_RPV9T2irG3J",
    "outputId": "1129a713-98ef-4734-a2ff-2980c5f073a3",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## run our function!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [
    "fvkdsW79RcUN"
   ],
   "name": "APAC-week-15 spacy_SOLUTIONS.ipynb",
   "provenance": [
    {
     "file_id": "1NHGImT0kxlFki3ktI66ZfpZDEMhIr94T",
     "timestamp": 1605060862393
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
